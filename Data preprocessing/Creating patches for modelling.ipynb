{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are based on those from the Luna16 tutorial and have been modified for my needs. https://luna16.grand-challenge.org/Tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now a function to:\n",
    "\n",
    "- Open the image \n",
    "- Store it into a numpy array\n",
    "- Extract the following info: Pixel Spacing, Origin\n",
    "\n",
    "This function takes as input the name of the image and returns:\n",
    "\n",
    "- The array corresponding to the image (numpyImage)\n",
    "- Origin (numpyOrigin)\n",
    "- PixelSpacing (numpySpacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_itk_image(filename):\n",
    "    itkimage = sitk.ReadImage(filename)\n",
    "    numpyImage = sitk.GetArrayFromImage(itkimage)\n",
    "     \n",
    "    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))\n",
    "    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "     \n",
    "    return numpyImage, numpyOrigin, numpySpacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the coordinates of the candidates are given in World Coordinates, we now need to transform from world coordinates to voxel coordinates. We define now a function to do that. Please note that the transformation below is only valid if there is no rotation component in the transformation matrix. For all CT images in our dataset, there is no rotation component so that this formula can be used. This function takes as inputs:\n",
    "\n",
    "- The world coordinates\n",
    "- The origin\n",
    "- The pixel Spacing\n",
    "\n",
    "This function returns:\n",
    "\n",
    "- Voxel coordinates (voxelCoord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worldToVoxelCoord(worldCoord, origin, spacing):\n",
    "     \n",
    "    stretchedVoxelCoord = np.absolute(worldCoord - origin)\n",
    "    voxelCoord = stretchedVoxelCoord / spacing\n",
    "    return voxelCoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract now some features from the candidates. We define some normalized planes to extract views from the candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizePlanes(npzarray):\n",
    "     \n",
    "    maxHU = 400.\n",
    "    minHU = -1000.\n",
    " \n",
    "    npzarray = (npzarray - minHU) / (maxHU - minHU)\n",
    "    npzarray[npzarray>1] = 1.\n",
    "    npzarray[npzarray<0] = 0.\n",
    "    return npzarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined these auxiliary functions, we can now define the main part of our script. First we:\n",
    "\n",
    "- Specificy the path where the file with the list of candidates is (cand_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data split to 'train', 'test', or 'validation'\n",
    "\n",
    "data_split = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_path = 'annotations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load candidates\n",
    "\n",
    "cands = pd.read_csv(cand_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354 files in directory\n"
     ]
    }
   ],
   "source": [
    "# get list of the images saved to disk\n",
    "\n",
    "ids = os.listdir(f'{data_split}_data_raw')\n",
    "print(f'{len(ids)} files in directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop filetype extension\n",
    "\n",
    "for i, val in enumerate(ids):\n",
    "    ids[i] = val[:-4]\n",
    "\n",
    "ids = list(pd.Series(ids).drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will not be using the entire dataset, I need to trim down the candidate list to only those values which I have saved on my disk. I will create a new df with only these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands_trim = pd.DataFrame(columns=cands.columns)\n",
    "\n",
    "for i, val in enumerate(ids):\n",
    "    cands_trim = cands_trim.append(cands.loc[cands.seriesuid == ids[i], :], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through list of trimmed candidates, load respective image file, extract patch of lung nodule, save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded 0\n",
      "Image loaded 1\n",
      "Image loaded 2\n",
      "Image loaded 3\n",
      "Image loaded 4\n",
      "Image loaded 5\n",
      "Image loaded 6\n",
      "Image loaded 7\n",
      "Image loaded 8\n",
      "Image loaded 9\n",
      "Image loaded 10\n",
      "Image loaded 11\n",
      "Image loaded 12\n",
      "Image loaded 13\n",
      "Image loaded 14\n",
      "Image loaded 15\n",
      "Image loaded 16\n",
      "Image loaded 17\n",
      "Image loaded 18\n",
      "Image loaded 19\n",
      "Image loaded 20\n",
      "Image loaded 21\n",
      "Image loaded 22\n",
      "Image loaded 23\n",
      "Image loaded 24\n",
      "Image loaded 25\n",
      "A null patch was found during iteration 25\n",
      "Image loaded 26\n",
      "A null patch was found during iteration 26\n",
      "Image loaded 27\n",
      "Image loaded 28\n",
      "Image loaded 29\n",
      "Image loaded 30\n",
      "Image loaded 31\n",
      "Image loaded 32\n",
      "Image loaded 33\n",
      "Image loaded 34\n",
      "Image loaded 35\n",
      "Image loaded 36\n",
      "Image loaded 37\n",
      "Image loaded 38\n",
      "Image loaded 39\n",
      "Image loaded 40\n",
      "Image loaded 41\n",
      "Image loaded 42\n",
      "Image loaded 43\n",
      "Image loaded 44\n",
      "Image loaded 45\n",
      "Image loaded 46\n",
      "Image loaded 47\n",
      "Image loaded 48\n",
      "Image loaded 49\n",
      "Image loaded 50\n",
      "Image loaded 51\n",
      "Image loaded 52\n",
      "Image loaded 53\n",
      "Image loaded 54\n",
      "Image loaded 55\n",
      "Image loaded 56\n",
      "Image loaded 57\n",
      "Image loaded 58\n",
      "Image loaded 59\n",
      "Image loaded 60\n",
      "Image loaded 61\n",
      "Image loaded 62\n",
      "Image loaded 63\n",
      "Image loaded 64\n",
      "Image loaded 65\n",
      "Image loaded 66\n",
      "Image loaded 67\n",
      "Image loaded 68\n",
      "Image loaded 69\n",
      "Image loaded 70\n",
      "Image loaded 71\n",
      "Image loaded 72\n",
      "Image loaded 73\n",
      "Image loaded 74\n",
      "Image loaded 75\n",
      "Image loaded 76\n",
      "A null patch was found during iteration 76\n",
      "Image loaded 77\n",
      "Image loaded 78\n",
      "Image loaded 79\n",
      "Image loaded 80\n",
      "Image loaded 81\n",
      "Image loaded 82\n",
      "Image loaded 83\n",
      "Image loaded 84\n",
      "Image loaded 85\n",
      "Image loaded 86\n",
      "Image loaded 87\n",
      "Image loaded 88\n",
      "Image loaded 89\n",
      "Image loaded 90\n",
      "Image loaded 91\n",
      "Image loaded 92\n",
      "Image loaded 93\n",
      "Image loaded 94\n",
      "Image loaded 95\n",
      "Image loaded 96\n",
      "Image loaded 97\n",
      "Image loaded 98\n",
      "Image loaded 99\n",
      "Image loaded 100\n",
      "Image loaded 101\n",
      "Image loaded 102\n",
      "Image loaded 103\n",
      "A null patch was found during iteration 103\n",
      "Image loaded 104\n",
      "Image loaded 105\n",
      "Image loaded 106\n",
      "Image loaded 107\n",
      "Image loaded 108\n",
      "Image loaded 109\n",
      "Image loaded 110\n",
      "Image loaded 111\n",
      "Image loaded 112\n",
      "Image loaded 113\n",
      "Image loaded 114\n",
      "Image loaded 115\n",
      "Image loaded 116\n",
      "Image loaded 117\n",
      "Image loaded 118\n",
      "Image loaded 119\n",
      "Image loaded 120\n",
      "Image loaded 121\n",
      "Image loaded 122\n",
      "Image loaded 123\n",
      "Image loaded 124\n",
      "Image loaded 125\n",
      "Image loaded 126\n",
      "Image loaded 127\n",
      "Image loaded 128\n",
      "Image loaded 129\n",
      "Image loaded 130\n",
      "Image loaded 131\n",
      "Image loaded 132\n",
      "Image loaded 133\n",
      "Image loaded 134\n",
      "Image loaded 135\n",
      "Image loaded 136\n",
      "Image loaded 137\n",
      "Image loaded 138\n",
      "Image loaded 139\n",
      "Image loaded 140\n",
      "Image loaded 141\n",
      "Image loaded 142\n",
      "Image loaded 143\n",
      "Image loaded 144\n",
      "Image loaded 145\n",
      "Image loaded 146\n",
      "Image loaded 147\n",
      "Image loaded 148\n",
      "Image loaded 149\n",
      "Image loaded 150\n",
      "Image loaded 151\n",
      "Image loaded 152\n",
      "Image loaded 153\n",
      "Image loaded 154\n",
      "Image loaded 155\n",
      "Image loaded 156\n",
      "Image loaded 157\n",
      "Image loaded 158\n",
      "Image loaded 159\n",
      "Image loaded 160\n",
      "Image loaded 161\n",
      "Image loaded 162\n",
      "Image loaded 163\n",
      "Image loaded 164\n",
      "Image loaded 165\n",
      "Image loaded 166\n",
      "Image loaded 167\n",
      "Image loaded 168\n",
      "Image loaded 169\n",
      "Image loaded 170\n",
      "Image loaded 171\n",
      "Image loaded 172\n",
      "Image loaded 173\n",
      "Image loaded 174\n",
      "Image loaded 175\n",
      "Image loaded 176\n",
      "Image loaded 177\n",
      "Image loaded 178\n",
      "Image loaded 179\n",
      "Image loaded 180\n",
      "Image loaded 181\n",
      "Image loaded 182\n",
      "Image loaded 183\n",
      "Image loaded 184\n",
      "Image loaded 185\n",
      "Image loaded 186\n",
      "Image loaded 187\n",
      "Image loaded 188\n",
      "Image loaded 189\n",
      "Image loaded 190\n",
      "Image loaded 191\n",
      "Image loaded 192\n",
      "Image loaded 193\n",
      "Image loaded 194\n",
      "Image loaded 195\n",
      "Image loaded 196\n",
      "Image loaded 197\n",
      "Image loaded 198\n",
      "Image loaded 199\n",
      "Image loaded 200\n",
      "Image loaded 201\n",
      "Image loaded 202\n",
      "Image loaded 203\n",
      "Image loaded 204\n",
      "Image loaded 205\n",
      "Image loaded 206\n",
      "Image loaded 207\n",
      "Image loaded 208\n",
      "Image loaded 209\n",
      "Image loaded 210\n",
      "Image loaded 211\n",
      "Image loaded 212\n",
      "Image loaded 213\n",
      "Image loaded 214\n",
      "Image loaded 215\n",
      "Image loaded 216\n",
      "Image loaded 217\n",
      "Image loaded 218\n",
      "Image loaded 219\n",
      "Image loaded 220\n",
      "Image loaded 221\n",
      "Image loaded 222\n",
      "Image loaded 223\n",
      "Image loaded 224\n",
      "Image loaded 225\n",
      "Image loaded 226\n",
      "Image loaded 227\n",
      "Image loaded 228\n",
      "That took 226.09 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(cands_trim.shape[0]):\n",
    "    \n",
    "    cand = cands_trim.loc[i,:]\n",
    "    voxelWidth = 65\n",
    "    \n",
    "    # Use a random offset to allow the model to identify the X, Y location\n",
    "    \n",
    "    offset_y = randint(-10, 10)\n",
    "    offset_x = randint(-10, 10)\n",
    "    \n",
    "    # load image\n",
    "    numpyImage, numpyOrigin, numpySpacing = load_itk_image(f'{data_split}_data_raw/{cand.seriesuid}.mhd')\n",
    "    if numpyImage.shape:\n",
    "        print(f'Image loaded {i}')\n",
    "    \n",
    "    # Create and save patch of lung nodule\n",
    "    worldCoord = np.asarray([float(cand.coordZ),float(cand.coordY),float(cand.coordX)])\n",
    "    voxelCoord = worldToVoxelCoord(worldCoord, numpyOrigin, numpySpacing)\n",
    "    \n",
    "    z = voxelCoord[0]\n",
    "    y = voxelCoord[1] + offset_y\n",
    "    x = voxelCoord[2] + offset_x\n",
    "    \n",
    "    try:\n",
    "        patch = numpyImage[int(z),\n",
    "                           int(y-voxelWidth/2):int(y+voxelWidth/2),\n",
    "                           int(x-voxelWidth/2):int(x+voxelWidth/2)]\n",
    "        \n",
    "        if patch.sum() != 0:\n",
    "            patch = normalizePlanes(patch)\n",
    "            Image.fromarray(patch*255).convert('L').save(f'{data_split}/pos/pos_{i}.tiff')\n",
    "            \n",
    "            cands_trim.loc[i, 'voxel_x'] = voxelWidth/2 - offset_x\n",
    "            cands_trim.loc[i, 'voxel_y'] = voxelWidth/2 - offset_y\n",
    "            cands_trim.loc[i, 'voxel_z'] = z\n",
    "        else:\n",
    "            print(f'A null patch was found during iteration {i}')\n",
    "            \n",
    "    except:\n",
    "        print(f'Error creating positive tile at iteration {i}')\n",
    "\n",
    "print(f'That took {time.time()-start_time:.2f} seconds to complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trimmed candidates list for modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands_trim.to_csv(f'candidates_{data_split}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the negative patches, to increase the probability that our negative patches do not have a nodule in them we will only take patches from series that only contain 1 nodule and modify the z coordinate. We will take a number of patches from each series based on the number of patches needed to keep a 1:1 positive to negative ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_nods = cands_trim.groupby('seriesuid').size().sort_values()\n",
    "single_nods = single_nods[single_nods == 1]\n",
    "n_per_series = int(cands_trim.shape[0]/len(single_nods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(n_per_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded 0\n",
      "Image loaded 1\n",
      "Image loaded 2\n",
      "Image loaded 3\n",
      "Image loaded 4\n",
      "Image loaded 5\n",
      "Image loaded 6\n",
      "Image loaded 7\n",
      "Image loaded 8\n",
      "Image loaded 9\n",
      "Image loaded 10\n",
      "Image loaded 11\n",
      "Image loaded 12\n",
      "Image loaded 13\n",
      "Image loaded 14\n",
      "Image loaded 15\n",
      "Image loaded 16\n",
      "Image loaded 17\n",
      "Image loaded 18\n",
      "Image loaded 19\n",
      "Image loaded 20\n",
      "Image loaded 21\n",
      "Image loaded 22\n",
      "Image loaded 23\n",
      "Error creating negative tile at iteration 23\n",
      "Image loaded 24\n",
      "Image loaded 25\n",
      "Error creating negative tile at iteration 25\n",
      "Image loaded 26\n",
      "Image loaded 27\n",
      "Image loaded 28\n",
      "A null patch was found during iteration 28\n",
      "A null patch was found during iteration 28\n",
      "A null patch was found during iteration 28\n",
      "A null patch was found during iteration 28\n",
      "Image loaded 29\n",
      "Image loaded 30\n",
      "Image loaded 31\n",
      "Image loaded 32\n",
      "Image loaded 33\n",
      "A null patch was found during iteration 33\n",
      "A null patch was found during iteration 33\n",
      "A null patch was found during iteration 33\n",
      "Error creating negative tile at iteration 33\n",
      "Image loaded 34\n",
      "Image loaded 35\n",
      "Error creating negative tile at iteration 35\n",
      "Image loaded 36\n",
      "Image loaded 37\n",
      "Error creating negative tile at iteration 37\n",
      "Image loaded 38\n",
      "Image loaded 39\n",
      "Error creating negative tile at iteration 39\n",
      "Image loaded 40\n",
      "Image loaded 41\n",
      "Image loaded 42\n",
      "A null patch was found during iteration 42\n",
      "A null patch was found during iteration 42\n",
      "A null patch was found during iteration 42\n",
      "A null patch was found during iteration 42\n",
      "Image loaded 43\n",
      "Image loaded 44\n",
      "Image loaded 45\n",
      "Image loaded 46\n",
      "Image loaded 47\n",
      "Image loaded 48\n",
      "Image loaded 49\n",
      "Image loaded 50\n",
      "That took 100.74 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i, seriesuid in enumerate(single_nods.index):\n",
    "    \n",
    "    cand = cands_trim.loc[cands_trim.seriesuid == seriesuid]\n",
    "    \n",
    "    # load image\n",
    "    numpyImage, numpyOrigin, numpySpacing = load_itk_image(f'{data_split}_data_raw/{seriesuid}.mhd')\n",
    "    if numpyImage.shape:\n",
    "        print(f'Image loaded {i}')\n",
    "        \n",
    "    worldCoord = np.asarray([float(cand.coordZ),float(cand.coordY),float(cand.coordX)])\n",
    "    voxelCoord = worldToVoxelCoord(worldCoord, numpyOrigin, numpySpacing)\n",
    "    \n",
    "    z = voxelCoord[0]\n",
    "    y = voxelCoord[1] + offset_y\n",
    "    x = voxelCoord[2] + offset_x\n",
    "        \n",
    "    for j in range(n_per_series):\n",
    "        \n",
    "        if z < 100:\n",
    "            z = z + 15*(j+1)\n",
    "        else:\n",
    "            z = z - 15*(j+1)\n",
    "    \n",
    "        try:\n",
    "            patch = numpyImage[int(z),\n",
    "                               int(y-voxelWidth/2):int(y+voxelWidth/2),\n",
    "                               int(x-voxelWidth/2):int(x+voxelWidth/2)]\n",
    "\n",
    "            if patch.sum() != 0:\n",
    "                patch = normalizePlanes(patch)\n",
    "                Image.fromarray(patch*255).convert('L').save(f'{data_split}/neg/neg_{count}.tiff')\n",
    "                count = count + 1\n",
    "\n",
    "            else:\n",
    "                print(f'A null patch was found during iteration {i}')\n",
    "\n",
    "        except:\n",
    "            print(f'Error creating negative tile at iteration {i}')\n",
    "    \n",
    "print(f'That took {time.time()-start_time:.2f} seconds to complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
